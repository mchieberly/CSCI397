Malachi Eberly
Assignment 5: Frozen Lake Q-Learning Report

For is_slippery = False, I got the following output:

Best reward updated 0.000 -> 1.000
Solved in 402 iterations!
Values:
+---------+--------+--------+---------+-------+
|   State |   Left |   Down |   Right |    Up |
+=========+========+========+=========+=======+
|       0 |      0 |  0     |     0   | 0     |
+---------+--------+--------+---------+-------+
|       1 |      0 |  0     |     0   | 0     |
+---------+--------+--------+---------+-------+
|       2 |      0 |  0.001 |     0   | 0     |
+---------+--------+--------+---------+-------+
|       3 |      0 |  0     |     0   | 0     |
+---------+--------+--------+---------+-------+
|       4 |      0 |  0     |     0   | 0     |
+---------+--------+--------+---------+-------+
|       5 |      0 |  0     |     0   | 0     |
+---------+--------+--------+---------+-------+
|       6 |      0 |  0.012 |     0   | 0     |
+---------+--------+--------+---------+-------+
|       7 |      0 |  0     |     0   | 0     |
+---------+--------+--------+---------+-------+
|       8 |      0 |  0     |     0   | 0     |
+---------+--------+--------+---------+-------+
|       9 |      0 |  0     |     0   | 0     |
+---------+--------+--------+---------+-------+
|      10 |      0 |  0.036 |     0   | 0.002 |
+---------+--------+--------+---------+-------+
|      11 |      0 |  0     |     0   | 0     |
+---------+--------+--------+---------+-------+
|      12 |      0 |  0     |     0   | 0     |
+---------+--------+--------+---------+-------+
|      13 |      0 |  0     |     0   | 0     |
+---------+--------+--------+---------+-------+
|      14 |      0 |  0     |     0.2 | 0     |
+---------+--------+--------+---------+-------+
|      15 |      0 |  0     |     0   | 0     |
+---------+--------+--------+---------+-------+

Policy:
Right  Right  Down   Left   
Left   HOLE   Down   HOLE   
Left   Left   Down   HOLE   
HOLE   Left   Right  GOAL   

Only one optimal path is found because once that path is found to be one 
that leads to the goal, the states on that path will always have the best 
value and will be taken. This is also why it goes from an average cumulative 
reward of 0 right to 1. Once it has a path, it always will take it.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

For is_slippery = True,  I got the following output:

Best reward updated 0.000 -> 0.300
Best reward updated 0.300 -> 0.350
Best reward updated 0.350 -> 0.400
Best reward updated 0.400 -> 0.450
Best reward updated 0.450 -> 0.500
Best reward updated 0.500 -> 0.550
Best reward updated 0.550 -> 0.600
Best reward updated 0.600 -> 0.650
Best reward updated 0.650 -> 0.700
Best reward updated 0.700 -> 0.750
Best reward updated 0.750 -> 0.800
Best reward updated 0.800 -> 0.850
Solved in 14098 iterations!
Values:
+---------+--------+--------+---------+-------+
|   State |   Left |   Down |   Right |    Up |
+=========+========+========+=========+=======+
|       0 |  0.07  |  0.069 |   0.069 | 0.061 |
+---------+--------+--------+---------+-------+
|       1 |  0.038 |  0.06  |   0.066 | 0.073 |
+---------+--------+--------+---------+-------+
|       2 |  0.092 |  0.086 |   0.077 | 0.069 |
+---------+--------+--------+---------+-------+
|       3 |  0.054 |  0.031 |   0.042 | 0.063 |
+---------+--------+--------+---------+-------+
|       4 |  0.098 |  0.064 |   0.09  | 0.023 |
+---------+--------+--------+---------+-------+
|       5 |  0     |  0     |   0     | 0     |
+---------+--------+--------+---------+-------+
|       6 |  0.081 |  0.041 |   0.119 | 0.019 |
+---------+--------+--------+---------+-------+
|       7 |  0     |  0     |   0     | 0     |
+---------+--------+--------+---------+-------+
|       8 |  0.066 |  0.066 |   0.102 | 0.152 |
+---------+--------+--------+---------+-------+
|       9 |  0.154 |  0.225 |   0.223 | 0.117 |
+---------+--------+--------+---------+-------+
|      10 |  0.225 |  0.202 |   0.138 | 0.128 |
+---------+--------+--------+---------+-------+
|      11 |  0     |  0     |   0     | 0     |
+---------+--------+--------+---------+-------+
|      12 |  0     |  0     |   0     | 0     |
+---------+--------+--------+---------+-------+
|      13 |  0.114 |  0.301 |   0.304 | 0.107 |
+---------+--------+--------+---------+-------+
|      14 |  0.316 |  0.524 |   0.373 | 0.618 |
+---------+--------+--------+---------+-------+
|      15 |  0     |  0     |   0     | 0     |
+---------+--------+--------+---------+-------+

Policy:
Left   Up     Left   Up     
Left   HOLE   Right  HOLE   
Up     Down   Left   HOLE   
HOLE   Right  Up     GOAL   

This looks a little different because stochasticity has been incorporated 
into the solution. At each state, the agent only has a 1/3 chance of 
moving to the intended state, so the policy is different to account for 
the possible perpendicular states it could transition to. This is also 
why there are more Q-values, a much higher number of iterations, and 
multiple best reward updates. 